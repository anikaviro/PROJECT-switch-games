{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f75a97",
   "metadata": {},
   "source": [
    "## Gathering Nintendo Switch game information from Metacritic\n",
    "\n",
    "First we scrape the metacritic site to find **a database of most Nintendo Switch games**. We chose this site because it has both users and critic score. We obtain the titles, release date, user score, critic score and the genre. Most fields can be extracted from the index page but for genre we have to navigate to each game site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13b1028c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 96\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata saved to nintendo_switch_games.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# and now we call the function to scrape game data\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[43mscrape_switch_games\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[63], line 85\u001b[0m, in \u001b[0;36mscrape_switch_games\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# call function to scrape data from all pages\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[43mscrape_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# create a dataframe from the scraped data\u001b[39;00m\n\u001b[1;32m     88\u001b[0m switch_games_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(data_dict)\n",
      "Cell \u001b[0;32mIn[63], line 81\u001b[0m, in \u001b[0;36mscrape_switch_games.<locals>.scrape_pages\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m content \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m num_loops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content)\n\u001b[0;32m---> 81\u001b[0m \u001b[43mscraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserAgent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[63], line 60\u001b[0m, in \u001b[0;36mscrape_switch_games.<locals>.scraper\u001b[0;34m(num_loops, content, userAgent)\u001b[0m\n\u001b[1;32m     58\u001b[0m game_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.metacritic.com\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     59\u001b[0m game_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(game_url, headers\u001b[38;5;241m=\u001b[39muserAgent)\n\u001b[0;32m---> 60\u001b[0m game_soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m genre_div \u001b[38;5;241m=\u001b[39m game_soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary_detail product_genre\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     62\u001b[0m genre \u001b[38;5;241m=\u001b[39m genre_div\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m genre_div \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Ironhack/Week_9/Final_project/PROJECT-switch-games/final_env/lib/python3.9/site-packages/bs4/__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Ironhack/Week_9/Final_project/PROJECT-switch-games/final_env/lib/python3.9/site-packages/bs4/__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[0;32m~/Desktop/Ironhack/Week_9/Final_project/PROJECT-switch-games/final_env/lib/python3.9/site-packages/bs4/builder/_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    378\u001b[0m parser\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserRejectedMarkup(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/html/parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/html/parser.py:170\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m, i):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen\u001b[38;5;241m.\u001b[39mmatch(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[1;32m    172\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/html/parser.py:344\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCDATA_CONTENT_ELEMENTS:\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_cdata_mode(tag)\n",
      "File \u001b[0;32m~/Desktop/Ironhack/Week_9/Final_project/PROJECT-switch-games/final_env/lib/python3.9/site-packages/bs4/builder/_htmlparser.py:151\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_starttag\u001b[0;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[1;32m    137\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\u001b[38;5;241m.\u001b[39mhandle_starttag(\n\u001b[1;32m    138\u001b[0m     name, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, attr_dict, sourceline\u001b[38;5;241m=\u001b[39msourceline,\n\u001b[1;32m    139\u001b[0m     sourcepos\u001b[38;5;241m=\u001b[39msourcepos\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mand\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mis_empty_element \u001b[38;5;129;01mand\u001b[39;00m handle_empty_element:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# events for tags of this name.\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_endtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_already_closed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# But we might encounter an explicit closing tag for this tag\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# later on. If so, we want to ignore it.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malready_closed_empty_element\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "File \u001b[0;32m~/Desktop/Ironhack/Week_9/Final_project/PROJECT-switch-games/final_env/lib/python3.9/site-packages/bs4/builder/_htmlparser.py:176\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_endtag\u001b[0;34m(self, name, check_already_closed)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malready_closed_empty_element\u001b[38;5;241m.\u001b[39mremove(name)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_endtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Ironhack/Week_9/Final_project/PROJECT-switch-games/final_env/lib/python3.9/site-packages/bs4/__init__.py:771\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_endtag\u001b[0;34m(self, name, nsprefix)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m#print(\"End tag: \" + name)\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popToTag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Ironhack/Week_9/Final_project/PROJECT-switch-games/final_env/lib/python3.9/site-packages/bs4/__init__.py:709\u001b[0m, in \u001b[0;36mBeautifulSoup._popToTag\u001b[0;34m(self, name, nsprefix, inclusivePop)\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    707\u001b[0m most_recently_popped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m stack_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagStack)\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(stack_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen_tag_counter\u001b[38;5;241m.\u001b[39mget(name):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_switch_games():\n",
    "    \"\"\"\n",
    "    Scrapes Nintendo Switch game data from Metacritic, including game name, release date, metascore, user score, and genre.\n",
    "\n",
    "    Returns a csv and a df with the stored data\n",
    "    \"\"\"\n",
    "\n",
    "    # define a dictionary to store scraped data\n",
    "    data_dict = {'name': [], 'release_date': [], 'metascore': [], 'user_score': [], 'genre': []}\n",
    "\n",
    "    # function to get the webpage\n",
    "    def webpage(pageNum):\n",
    "        url = 'https://www.metacritic.com/browse/games/score/metascore/all/switch/filtered?sort=desc&view=detailed&page=' + str(pageNum)\n",
    "        userAgent = {'User-agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=userAgent)\n",
    "        return response\n",
    "\n",
    "    # function to determine the number of pages\n",
    "    def numberPages(response):\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        pages = soup.find_all('li', {\"class\": \"active_page\"})\n",
    "        if pages:\n",
    "            pagesCleaned = pages[0].find('span', {\"class\": \"page_num\"})\n",
    "            return int(pagesCleaned.text)\n",
    "        return 0\n",
    "\n",
    "    # with this we scrape game data from each table\n",
    "    def scraper(num_loops, content, userAgent):\n",
    "        tblnum = 0\n",
    "        while tblnum < num_loops:\n",
    "            table_rows = content[tblnum].find_all('tr')\n",
    "            for tr in table_rows:\n",
    "                if len(tr) < 1:\n",
    "                    continue\n",
    "                td = tr.find_all('td')\n",
    "                a = td[1].find('a', {\"class\": \"title\"})\n",
    "                data_dict['name'].append(a.find('h3').text.strip())\n",
    "\n",
    "                # game release date\n",
    "                date = td[1].find('span', {\"class\": \"\"})\n",
    "                data_dict['release_date'].append(date.text.strip())\n",
    "\n",
    "                # getting the user score\n",
    "                div_score = td[1].find('div', {\"class\": \"clamp-userscore\"})\n",
    "                user = div_score.find('div', {\"class\": \"metascore_w\"})\n",
    "                data_dict['user_score'].append(user.text.strip())\n",
    "\n",
    "                # getting the metascore\n",
    "                score = td[1].find('div', {\"class\": \"metascore_w\"})\n",
    "                data_dict['metascore'].append(score.text.strip())\n",
    "\n",
    "                # with this we get the genre from each game detail page\n",
    "                game_url = 'https://www.metacritic.com' + a['href']\n",
    "                game_response = requests.get(game_url, headers=userAgent)\n",
    "                game_soup = BeautifulSoup(game_response.text, 'html.parser')\n",
    "                genre_div = game_soup.find('li', class_='summary_detail product_genre')\n",
    "                genre = genre_div.find('span', class_='data').text.strip() if genre_div else 'N/A'\n",
    "                data_dict['genre'].append(genre)\n",
    "\n",
    "            tblnum += 1\n",
    "\n",
    "    # iterating over pages and scraping game data\n",
    "    def scrape_pages():\n",
    "        pgs = list(range(0, 199))\n",
    "        for pg in pgs:\n",
    "            numPage = numberPages(webpage(pg))\n",
    "            if numPage == 0:\n",
    "                break\n",
    "            currentPage = pg\n",
    "            url = 'https://www.metacritic.com/browse/games/score/metascore/all/switch/filtered?page=' + str(currentPage)\n",
    "            userAgent = {'User-agent': 'Mozilla/5.0'}\n",
    "            response = requests.get(url, headers=userAgent)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            content = soup.find_all('table')\n",
    "            num_loops = len(content)\n",
    "            scraper(num_loops, content, userAgent)\n",
    "            time.sleep(5)\n",
    "\n",
    "    # call function to scrape data from all pages\n",
    "    scrape_pages()\n",
    "\n",
    "    # create a dataframe from the scraped data\n",
    "    switch_games_df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "    # save to csv \n",
    "    switch_games_df.to_csv('nintendo_switch_games.csv', index=False)\n",
    "    print(\"data saved to nintendo_switch_games.csv\")  \n",
    "\n",
    "\n",
    "# and now we call the function to scrape game data\n",
    "scrape_switch_games()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_env",
   "language": "python",
   "name": "final_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
